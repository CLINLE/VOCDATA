# Vocational Data Platform
Starter repository for my master thesis.
# VOCDATA ‚Äì Datenarchitektur & -Pipeline f√ºr BFS-Berufsbildungsstatistik  
Masterarbeit Claudia Ledenig (ZHAW)

## 1 √úberblick
Die Arbeit beweist den Mehrwert eines integrativen Datensystems (qualitative und quantitative daten) anhand einer effizienteren Auswertung von Berufsbildungsdaten, konkret **Lehrvertrags¬≠abbr√ºchen**.  

Dieses Repository enth√§lt aktuell den vollst√§ndigen, reproduzierbaren **ETL-Prozess** f√ºr zwei amtliche Datens√§tze des Bundesamts f√ºr Statistik (BFS):

1. **Lehrvertragsaufl√∂sungen (LVA) ‚Äì Kohorte 2019**  
   *Beobachtungszeitraum: Lehrbeginn 2019 bis 31.12.2023*

2. **Abschlussquoten Sek II ‚Äì Referenzjahr 2022**

Ziel ist ein **Sternschema** in MySQL 8, √ºber das sich beide Datent√∂pfe konsistent analysieren lassen (Plotly-Dashboards, Power BI o. √Ñ.).  
Alle Schritte sind in Jupyter-Notebooks dokumentiert und versioniert.

---

## 2 Ordnerstruktur 
vvocdata/
‚îú‚îÄ‚îÄ data/                # Roh-Excel & CSV-Quellen
‚îú‚îÄ‚îÄ tmp/                 # Parquet-Staging + Audit-CSVs
‚îú‚îÄ‚îÄ uploads/
‚îÇ   ‚îî‚îÄ‚îÄ pdf/             # Manuell hochgeladene PDF-Dokumente
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   01a_profile_lva.ipynb
‚îÇ   01b_profile_abschluss.ipynb
‚îÇ   02a_clean_lva.ipynb
‚îÇ   02b_clean_abschluss.ipynb
‚îÇ   04a_load_facts_lva.ipynb
‚îÇ   04b_load_facts_abschluss.ipynb
‚îÇ   05_fk_updates.ipynb
‚îÇ   06a_lva_audit.ipynb
‚îÇ   06b_abs_audit.ipynb
‚îÇ   07a_lva_quality.ipynb
‚îÇ   07b_abs_quality.ipynb
‚îÇ   90_qual_text_search.ipynb      # NEU: ChatGPT-Suche / Zusammenfassung
‚îú‚îÄ‚îÄ scripts/
‚îÇ   pdf_ingest.py          # PDF ‚Üí Text ‚Üí qual_docs-Tabelle
‚îÇ   etl_qual_docs.py       # Hilfsfunktionen f√ºr Text-ETL
‚îÇ   db_models.py           # SQLAlchemy-ORM (Dim/Facts/qual_docs)
‚îÇ   config.py              # Liest .env (OpenAI-Key, DB-Creds)
‚îú‚îÄ‚îÄ tests/                 # (optional) kleine Py-Tests f√ºr ETL-Funktionen
‚îú‚îÄ‚îÄ .env                   # **nicht committen** ‚Äì API-Keys, DB-Passw√∂rter
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md





## 3 Voraussetzungen

| Komponente | Version | Kurzinstallation |
|------------|---------|------------------|
| **Miniconda 3** | ‚â• 23.x | `conda create -n vocdata python=3.11` |
| **VS Code**    | ‚â• 1.88 | Python- & Jupyter-Extension aktiv |
| **Docker Desktop** | ‚â• 28.x | WSL 2 Backend, 4 GB RAM gen√ºgen |
| **MySQL 8 Container** | `mysql:8` | `docker run -d --name voc-mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=voc_root mysql:8` |
| **MySQL Workbench** | ‚â• 8.0.36 | optional ‚Äì ER-Diagramm |

---

## 4 Schnellstart (End-to-End)

```bash
# 1. Umgebung aktivieren
conda activate vocdata
pip install -r requirements.txt   # pandas, SQLAlchemy, plotly, ‚Ä¶

# 2. JupyterLab starten
jupyter lab

# 3. Notebooks sequenziell ausf√ºhren
#    a_profile/02_profile_bfs_lva.ipynb  (nur Sichtpr√ºfung)
#    b_ETL/04_load_dims.ipynb            (Dimensionen schreiben)
#    b_ETL/05_load_facts.ipynb           (Kohorte 2019 LVA-Fakt)
#    a_profile/03_profile_bfs_abschlussquote.ipynb
#    b_ETL/08_load_fact_abschluss_stats.ipynb


Das Schema vocdata wird automatisch erstellt (Container-Root-Login).
Nach erfolgreichem Lauf liefert SELECT COUNT(*) FROM fact_lva_stats; 1188 Zeilen.

5 Notebook-Landkarte
Notebook	Zweck	Schreibt SQL?
01_poc_overview	Mini-End-to-End mit Demo-CSV und Plotly	‚úî (bfs_demo)
02_profile_bfs_lva	Spaltencheck, Kardinalit√§t, NULL-Anteile	‚úñ
03_profile_bfs_abschlussquote	dito	‚úñ
04_load_dims	13 Dimensionstabellen (inkl. dim_kohorte)	‚úî
05_load_facts	fact_lva_stats inkl. Flags + Kohorte	‚úî
07_check_abschlussquote	Audit‚ÄêNotebook f√ºr Abschlussfakt	‚úñ
08_load_fact_abschluss_stats	fact_abschluss_stats schreiben	‚úî

6 ER-Diagramm
docs/diagramme/erm_v1.png zeigt das aktuelle Sternschema.
Die Tabellen sind in Workbench in zwei Gruppen gefasst:

LVA-Kohorte 2019 (gr√ºn)

Abschlussquoten 2022 (blau)

7 To Do (Stand 23 Jun 2025)
 Pfade im Code auf relative ../data/‚Ä¶ pr√ºfen

 README Methodik fertigschreiben (Kap 4 Thesis)

 PX-Dateien testweise integrieren (falls Zeitbudget)

 Qualit√§ts-Notebook: FK-Checks, Summenkontrolle

8 Lizenz
Nur f√ºr Lehr- und Forschungszwecke der ZHAW; Rohdaten ¬© BFS.
Quellcode unter MIT-Lizenz.





_____
PDF-Upload & Ingestion ‚Äì 3 ‚Äì 4 h

ETL-Pipeline qualitative Texte (qual_docs) ‚Äì 1 ‚Äì 2 h

ChatGPT-Integration f√ºr Textanalyse ‚Äì 1 ‚Äì 2 h

Such-/Zusammenfassungsfunktion in Jupyter ‚Äì 2 ‚Äì 3 h



PDF-Upload & Ingestion
# Implementierung & Technische Entscheidungen (Stand MVP)

## 1 Conda-Umgebung `vocdata` aktiviert

* **Ziel / Problem** Reproduzierbare Laufzeit f√ºr alle ETL-Skripte.
* **Begr√ºndung** Conda bietet paketgenaue Isolation; Umgebung war bereits vorbereitet.
* **Ergebnis / Nachweis** Nach `conda activate vocdata` erscheint der Prompt `(vocdata)`.

---

## 2 Installation **PyPDF2**

* **Ziel** Textextraktion aus PDFs f√ºr den Import qualitativer Quellen.
* **Begr√ºndung** Reine-Python-Bibliothek, keine externen Services n√∂tig, schnell genug f√ºr Klartext.
  *Alternativen:* pdfminer (langsamer) oder Apache Tika (Java-Abh√§ngigkeit) ‚Äì verworfen.
* **Ergebnis** `pip install PyPDF2` ‚Üí *Successfully installed PyPDF2-3.0.1*.

---

## 3 Ordner *uploads*, *uploads/pdf*, *scripts* angelegt

* **Ziel** Saubere Trennung zwischen Roh-Uploads und wieder¬≠verwendbarem Code.
* **Begr√ºndung** Verhindert Datenchaos, erleichtert Git-Diffs.
* **Ergebnis** Ordner committed (Git-Hash `dc51e19`).

---

## 4 Beispiel-PDFs hinzugef√ºgt

Acht PDFs unter *uploads/pdf/* als Testmaterial f√ºr ETL und sp√§tere NLP-Analyse.

---

## 5 MySQL-Tabelle `qual_docs` erstellt

```sql
CREATE TABLE IF NOT EXISTS vocdata.qual_docs (
    doc_id    INT AUTO_INCREMENT PRIMARY KEY,
    filename  VARCHAR(255) NOT NULL,
    title     VARCHAR(255),
    year      SMALLINT,
    full_text LONGTEXT,
    added_at  TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

* **Ziel** Zentrale Ablage aller PDF-Volltexte ‚Äì Basis f√ºr Suche / LLM.
* **Begr√ºndung** Einfaches Schema mit AUTO\_INCREMENT-PK; `LONGTEXT` deckt gro√üe Studien ab.
* **Ergebnis** MySQL meldet *Query OK*.

---

## 6 Skript `scripts/pdf_ingest.py` erstellt

* **Ziel** Automatisierte Ingestion: PDF ‚Üí Text ‚Üí MySQL (`qual_docs`).
* **Begr√ºndung** Skript (nicht Notebook), damit unbeaufsichtigt per Cron‚Äâ/‚ÄâCI startbar.
* **Technik** PyPDF2-Parsing, `sqlalchemy+pymysql`-Insert, `glob`‚ÄêDateiscan.
* **Fix** SQLAlchemy-URL auf `?charset=utf8mb4` umgestellt.

---

## 7 Installation **PyCryptodome**

* **Ziel** Unterst√ºtzung f√ºr AES-verschl√ºsselte PDFs (PyPDF2-Requirement).
* **Ergebnis** `pip install pycryptodome` ‚Äì keine Fehler.

---

## 8 Installation **ipywidgets**

* **Ziel** Interaktive Datei-Uploads direkt im Notebook.
* **Begr√ºndung** Jupyter-natives Widget, kein externes Front-End n√∂tig.
* **Ergebnis** `pip install ipywidgets` erfolgreich; VS Code unterst√ºtzt Widgets sofort.

---

## Upload-Widget & Bereitstellung (fortgeschrittene Schritte)

9. **scripts/__init__.py angelegt**  
   - macht das Verzeichnis *scripts* als Python-Paket importierbar.

10. **Notebook *notebooks/widgets/upload_widget.ipynb* erstellt**  
    - Upload-Button (ipywidgets 8) + Sofort-Ingestion in `qual_docs`.  
    - Pfad-Fix:  
      ```python
      sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
      ```

11. **Event-Handler f√ºr ipywidgets 8 angepasst**  
    - `change["new"]` liefert ein *Tuple* ‚Üí kein `.items()` mehr.

12. **Minimaler, aufger√§umter Widget-Code eingef√ºhrt**  
    - zwei Zellen gen√ºgen: *Setup & Widget*, *optional: Dateiliste*.  
    - Erfolgs¬≠meldung: **üöÄ Upload & Ingestion fertig.**

13. **Kurzer Listing-Snippet**  
    ```python
    for p in PDF_DIR.glob("*.pdf"):
        print(" -", p.name)
    ```

14. **Daten¬≠bereitstellung f√ºr den Dozenten gekl√§rt**  
    - Empfehlung: *Starter-Paket* ‚Üí SQL-Seed + Beispiel-PDFs werden beim ersten `docker compose up` automatisch importiert.  
    - Alternative Wege (ZIP-Volume, Managed Cloud-DB) kurz erl√§utert.

### OpenAI-Integration & Code-Struktur

15. **scripts/config.py** erstellt  
    - L√§dt `.env`, stellt `OPENAI_KEY` zentral bereit.

16. **.env** im Projektstamm angelegt (nicht im Repo):  
    `OPENAI_API_KEY=sk-‚Ä¶`

17. **Ordnerstruktur verfeinert**  
    ```
    scripts/
    ‚îú‚îÄ pdf_upload/ingest.py        # ETL f√ºr PDFs
    ‚îî‚îÄ chatgpt_config/summarize_pdf_files.py
    ```
    `__init__.py` in allen Unterordnern, damit Python-Pakete.

18. **summarize_pdf_files.py** (neue OpenAI v1-Syntax)  
    - Funktion `summarize(doc_id)` holt Volltext aus `qual_docs`, ruft `gpt-4o-mini` auf, liefert 8-Satz-Summary.

19. **Bibliothek aktualisiert:** `pip install --upgrade openai python-dotenv`

20. **Funktions¬≠test:**  
    ```powershell
    python -c "from scripts.chatgpt_config.summarize_pdf_files import summarize; print(summarize(1)[:400])"
    ```  
    ‚Üí Zusammenfassung erscheint ‚áí Integration erfolgreich.

### Volltext-Suche & Unit-Tests

21. **FULLTEXT-Index angelegt**  
    ```sql
    ALTER TABLE qual_docs
    ADD FULLTEXT idx_full_text (full_text);
    ```

22. **search-Modul erstellt**  
    *scripts/chatgpt_config/search.py*  
    ```python
    def search(term, limit=20):
        SELECT doc_id, filename,
               SUBSTRING(full_text,1,300) AS snippet
        FROM qual_docs
        WHERE MATCH(full_text) AGAINST (:q IN NATURAL LANGUAGE MODE)
        LIMIT :lim
    ```

23. **Test-Infrastruktur eingerichtet**  
    - Ordner **tests/** auf Root-Ebene  
    - `test_summarize.py` mit Pfad-Fix + `assert summarize(1)`  
    - Aufruf: `python -m pytest -q` ‚Üí **1 passed**

24. **Pytest in Conda-Env ausgef√ºhrt**  
    Verhindert Modul¬≠fehler (`pymysql`) durch falschen Interpreter.

### Suche & Auswahl-Widget (Notebook)

25. **Notebook `search_widget.ipynb` angelegt**  
    - Pfad-Fix (`sys.path.insert`) erg√§nzt.  
    - Importiert jetzt  
      ```python
      from chatgpt_integration.search_widget import search
      from chatgpt_integration.summarize_widget import summarize
      ```

26. **Fehlerbehebung**  
    - Module `search_widget.py` und `summarize_widget.py` liegen unter `scripts/chatgpt_integration/`.  
    
      ```  
      brauchen noch l√∂sung so dass kein `ModuleNotFoundError: scripts.config` mehr auftritt.



