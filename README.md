# Vocational Data Platform
Starter repository for my master thesis.
# VOCDATA â€“ Datenarchitektur & -Pipeline fÃ¼r BFS-Berufsbildungsstatistik  
Masterarbeit Claudia Ledenig (ZHAW)

## 1 Ãœberblick
Die Arbeit beweist den Mehrwert eines integrativen Datensystems (qualitative und quantitative daten) anhand einer effizienteren Auswertung von Berufsbildungsdaten, konkret **LehrvertragsÂ­abbrÃ¼chen**.  

Dieses Repository enthÃ¤lt aktuell den vollstÃ¤ndigen, reproduzierbaren **ETL-Prozess** fÃ¼r zwei amtliche DatensÃ¤tze des Bundesamts fÃ¼r Statistik (BFS):

1. **LehrvertragsauflÃ¶sungen (LVA) â€“ Kohorte 2019**  
   *Beobachtungszeitraum: Lehrbeginn 2019 bis 31.12.2023*

2. **Abschlussquoten Sek II â€“ Referenzjahr 2022**

Ziel ist ein **Sternschema** in MySQL 8, Ã¼ber das sich beide DatentÃ¶pfe konsistent analysieren lassen (Plotly-Dashboards, Power BI o. Ã„.).  
Alle Schritte sind in Jupyter-Notebooks dokumentiert und versioniert.

---

## 2 Ordnerstruktur 
vvocdata/
â”œâ”€â”€ data/                # Roh-Excel & CSV-Quellen
â”œâ”€â”€ tmp/                 # Parquet-Staging + Audit-CSVs
â”œâ”€â”€ uploads/
â”‚   â””â”€â”€ pdf/             # Manuell hochgeladene PDF-Dokumente
â”œâ”€â”€ notebooks/
â”‚   01a_profile_lva.ipynb
â”‚   01b_profile_abschluss.ipynb
â”‚   02a_clean_lva.ipynb
â”‚   02b_clean_abschluss.ipynb
â”‚   04a_load_facts_lva.ipynb
â”‚   04b_load_facts_abschluss.ipynb
â”‚   05_fk_updates.ipynb
â”‚   06a_lva_audit.ipynb
â”‚   06b_abs_audit.ipynb
â”‚   07a_lva_quality.ipynb
â”‚   07b_abs_quality.ipynb
â”‚   90_qual_text_search.ipynb      # NEU: ChatGPT-Suche / Zusammenfassung
â”œâ”€â”€ scripts/
â”‚   pdf_ingest.py          # PDF â†’ Text â†’ qual_docs-Tabelle
â”‚   etl_qual_docs.py       # Hilfsfunktionen fÃ¼r Text-ETL
â”‚   db_models.py           # SQLAlchemy-ORM (Dim/Facts/qual_docs)
â”‚   config.py              # Liest .env (OpenAI-Key, DB-Creds)
â”œâ”€â”€ tests/                 # (optional) kleine Py-Tests fÃ¼r ETL-Funktionen
â”œâ”€â”€ .env                   # **nicht committen** â€“ API-Keys, DB-PasswÃ¶rter
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md





## 3 Voraussetzungen

| Komponente | Version | Kurzinstallation |
|------------|---------|------------------|
| **Miniconda 3** | â‰¥ 23.x | `conda create -n vocdata python=3.11` |
| **VS Code**    | â‰¥ 1.88 | Python- & Jupyter-Extension aktiv |
| **Docker Desktop** | â‰¥ 28.x | WSL 2 Backend, 4 GB RAM genÃ¼gen |
| **MySQL 8 Container** | `mysql:8` | `docker run -d --name voc-mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=voc_root mysql:8` |
| **MySQL Workbench** | â‰¥ 8.0.36 | optional â€“ ER-Diagramm |

---

## 4 Schnellstart (End-to-End)

```bash
# 1. Umgebung aktivieren
conda activate vocdata
pip install -r requirements.txt   # pandas, SQLAlchemy, plotly, â€¦

# 2. JupyterLab starten
jupyter lab

# 3. Notebooks sequenziell ausfÃ¼hren
#    a_profile/02_profile_bfs_lva.ipynb  (nur SichtprÃ¼fung)
#    b_ETL/04_load_dims.ipynb            (Dimensionen schreiben)
#    b_ETL/05_load_facts.ipynb           (Kohorte 2019 LVA-Fakt)
#    a_profile/03_profile_bfs_abschlussquote.ipynb
#    b_ETL/08_load_fact_abschluss_stats.ipynb


Das Schema vocdata wird automatisch erstellt (Container-Root-Login).
Nach erfolgreichem Lauf liefert SELECT COUNT(*) FROM fact_lva_stats; 1188 Zeilen.

5 Notebook-Landkarte
Notebook	Zweck	Schreibt SQL?
01_poc_overview	Mini-End-to-End mit Demo-CSV und Plotly	âœ” (bfs_demo)
02_profile_bfs_lva	Spaltencheck, KardinalitÃ¤t, NULL-Anteile	âœ–
03_profile_bfs_abschlussquote	dito	âœ–
04_load_dims	13 Dimensionstabellen (inkl. dim_kohorte)	âœ”
05_load_facts	fact_lva_stats inkl. Flags + Kohorte	âœ”
07_check_abschlussquote	Auditâ€Notebook fÃ¼r Abschlussfakt	âœ–
08_load_fact_abschluss_stats	fact_abschluss_stats schreiben	âœ”

6 ER-Diagramm
docs/diagramme/erm_v1.png zeigt das aktuelle Sternschema.
Die Tabellen sind in Workbench in zwei Gruppen gefasst:

LVA-Kohorte 2019 (grÃ¼n)

Abschlussquoten 2022 (blau)

7 To Do (Stand 23 Jun 2025)
 Pfade im Code auf relative ../data/â€¦ prÃ¼fen

 README Methodik fertigschreiben (Kap 4 Thesis)

 PX-Dateien testweise integrieren (falls Zeitbudget)

 QualitÃ¤ts-Notebook: FK-Checks, Summenkontrolle

8 Lizenz
Nur fÃ¼r Lehr- und Forschungszwecke der ZHAW; Rohdaten Â© BFS.
Quellcode unter MIT-Lizenz.





_____
PDF-Upload & Ingestion â€“ 3 â€“ 4 h

ETL-Pipeline qualitative Texte (qual_docs) â€“ 1 â€“ 2 h

ChatGPT-Integration fÃ¼r Textanalyse â€“ 1 â€“ 2 h

Such-/Zusammenfassungsfunktion in Jupyter â€“ 2 â€“ 3 h



PDF-Upload & Ingestion
# Implementierung & Technische Entscheidungen (Stand MVP)

## 1 Conda-Umgebung `vocdata` aktiviert

* **Ziel / Problem** Reproduzierbare Laufzeit fÃ¼r alle ETL-Skripte.
* **BegrÃ¼ndung** Conda bietet paketgenaue Isolation; Umgebung war bereits vorbereitet.
* **Ergebnis / Nachweis** Nach `conda activate vocdata` erscheint der Prompt `(vocdata)`.

---

## 2 Installation **PyPDF2**

* **Ziel** Textextraktion aus PDFs fÃ¼r den Import qualitativer Quellen.
* **BegrÃ¼ndung** Reine-Python-Bibliothek, keine externen Services nÃ¶tig, schnell genug fÃ¼r Klartext.
  *Alternativen:* pdfminer (langsamer) oder Apache Tika (Java-AbhÃ¤ngigkeit) â€“ verworfen.
* **Ergebnis** `pip install PyPDF2` â†’ *Successfully installed PyPDF2-3.0.1*.

---

## 3 Ordner *uploads*, *uploads/pdf*, *scripts* angelegt

* **Ziel** Saubere Trennung zwischen Roh-Uploads und wiederÂ­verwendbarem Code.
* **BegrÃ¼ndung** Verhindert Datenchaos, erleichtert Git-Diffs.
* **Ergebnis** Ordner committed (Git-Hash `dc51e19`).

---

## 4 Beispiel-PDFs hinzugefÃ¼gt

Acht PDFs unter *uploads/pdf/* als Testmaterial fÃ¼r ETL und spÃ¤tere NLP-Analyse.

---

## 5 MySQL-Tabelle `qual_docs` erstellt

```sql
CREATE TABLE IF NOT EXISTS vocdata.qual_docs (
    doc_id    INT AUTO_INCREMENT PRIMARY KEY,
    filename  VARCHAR(255) NOT NULL,
    title     VARCHAR(255),
    year      SMALLINT,
    full_text LONGTEXT,
    added_at  TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

* **Ziel** Zentrale Ablage aller PDF-Volltexte â€“ Basis fÃ¼r Suche / LLM.
* **BegrÃ¼ndung** Einfaches Schema mit AUTO\_INCREMENT-PK; `LONGTEXT` deckt groÃŸe Studien ab.
* **Ergebnis** MySQL meldet *Query OK*.

---

## 6 Skript `scripts/pdf_ingest.py` erstellt

* **Ziel** Automatisierte Ingestion: PDF â†’ Text â†’ MySQL (`qual_docs`).
* **BegrÃ¼ndung** Skript (nicht Notebook), damit unbeaufsichtigt per Cronâ€‰/â€‰CI startbar.
* **Technik** PyPDF2-Parsing, `sqlalchemy+pymysql`-Insert, `glob`â€Dateiscan.
* **Fix** SQLAlchemy-URL auf `?charset=utf8mb4` umgestellt.

---

## 7 Installation **PyCryptodome**

* **Ziel** UnterstÃ¼tzung fÃ¼r AES-verschlÃ¼sselte PDFs (PyPDF2-Requirement).
* **Ergebnis** `pip install pycryptodome` â€“ keine Fehler.

---

## 8 Installation **ipywidgets**

* **Ziel** Interaktive Datei-Uploads direkt im Notebook.
* **BegrÃ¼ndung** Jupyter-natives Widget, kein externes Front-End nÃ¶tig.
* **Ergebnis** `pip install ipywidgets` erfolgreich; VS Code unterstÃ¼tzt Widgets sofort.

---

## Upload-Widget & Bereitstellung (fortgeschrittene Schritte)

9. **scripts/__init__.py angelegt**  
   - macht das Verzeichnis *scripts* als Python-Paket importierbar.

10. **Notebook *notebooks/widgets/upload_widget.ipynb* erstellt**  
    - Upload-Button (ipywidgets 8) + Sofort-Ingestion in `qual_docs`.  
    - Pfad-Fix:  
      ```python
      sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
      ```

11. **Event-Handler fÃ¼r ipywidgets 8 angepasst**  
    - `change["new"]` liefert ein *Tuple* â†’ kein `.items()` mehr.

12. **Minimaler, aufgerÃ¤umter Widget-Code eingefÃ¼hrt**  
    - zwei Zellen genÃ¼gen: *Setup & Widget*, *optional: Dateiliste*.  
    - ErfolgsÂ­meldung: **ðŸš€ Upload & Ingestion fertig.**

13. **Kurzer Listing-Snippet**  
    ```python
    for p in PDF_DIR.glob("*.pdf"):
        print(" -", p.name)
    ```

14. **DatenÂ­bereitstellung fÃ¼r den Dozenten geklÃ¤rt**  
    - Empfehlung: *Starter-Paket* â†’ SQL-Seed + Beispiel-PDFs werden beim ersten `docker compose up` automatisch importiert.  
    - Alternative Wege (ZIP-Volume, Managed Cloud-DB) kurz erlÃ¤utert.

### OpenAI-Integration & Code-Struktur

15. **scripts/config.py** erstellt  
    - LÃ¤dt `.env`, stellt `OPENAI_KEY` zentral bereit.

16. **.env** im Projektstamm angelegt (nicht im Repo):  
    `OPENAI_API_KEY=sk-â€¦`

17. **Ordnerstruktur verfeinert**  
    ```
    scripts/
    â”œâ”€ pdf_upload/ingest.py        # ETL fÃ¼r PDFs
    â””â”€ chatgpt_config/summarize_pdf_files.py
    ```
    `__init__.py` in allen Unterordnern, damit Python-Pakete.

18. **summarize_pdf_files.py** (neue OpenAI v1-Syntax)  
    - Funktion `summarize(doc_id)` holt Volltext aus `qual_docs`, ruft `gpt-4o-mini` auf, liefert 8-Satz-Summary.

19. **Bibliothek aktualisiert:** `pip install --upgrade openai python-dotenv`

20. **FunktionsÂ­test:**  
    ```powershell
    python -c "from scripts.chatgpt_config.summarize_pdf_files import summarize; print(summarize(1)[:400])"
    ```  
    â†’ Zusammenfassung erscheint â‡’ Integration erfolgreich.

### Volltext-Suche & Unit-Tests

21. **FULLTEXT-Index angelegt**  
    ```sql
    ALTER TABLE qual_docs
    ADD FULLTEXT idx_full_text (full_text);
    ```

22. **search-Modul erstellt**  
    *scripts/chatgpt_config/search.py*  
    ```python
    def search(term, limit=20):
        SELECT doc_id, filename,
               SUBSTRING(full_text,1,300) AS snippet
        FROM qual_docs
        WHERE MATCH(full_text) AGAINST (:q IN NATURAL LANGUAGE MODE)
        LIMIT :lim
    ```

23. **Test-Infrastruktur eingerichtet**  
    - Ordner **tests/** auf Root-Ebene  
    - `test_summarize.py` mit Pfad-Fix + `assert summarize(1)`  
    - Aufruf: `python -m pytest -q` â†’ **1 passed**

24. **Pytest in Conda-Env ausgefÃ¼hrt**  
    Verhindert ModulÂ­fehler (`pymysql`) durch falschen Interpreter.

### Suche & Auswahl-Widget (Notebook)

25. **Notebook `search_widget.ipynb` angelegt**  
    - Pfad-Fix (`sys.path.insert`) ergÃ¤nzt.  
    - Importiert jetzt  
      ```python
      from chatgpt_integration.search_widget import search
      from chatgpt_integration.summarize_widget import summarize
      ```

26. **Fehlerbehebung**  
    - Module `search_widget.py` und `summarize_widget.py` liegen unter `scripts/chatgpt_integration/`.  
    
      ```  
      brauchen noch lÃ¶sung so dass kein `ModuleNotFoundError: scripts.config` mehr auftritt.



Notes Search funktion 2 Level
1.  Suche (search(term)) â€“ reines SQL / FTS in MySQL (LÃ¤uft lokal in der DB-Engine.)
2. Kontext â†’ LLM â€“ Ausschnitte (snippet oder full_text[:12000]) werden zusammen mit der User-Frage an gpt-4o-mini geschickt (RAG-Aufruf, summarize() etc.)

Ablauf
Suchergebnis â†’ Prompt
search() findet relevante Textstellen, z. B. 3 Ã— 400 WÃ¶rter.
Diese Ausschnitte wandern in den System-/Kontext-Teil der Chat-Nachricht.

PromptlÃ¤nge = Tokenverbrauch
Je mehr Treffer / lÃ¤ngere Ausschnitte, desto mehr Input-Tokens.
Jeder zusÃ¤tzliche Token kostet (Stand Juni 2025) z. B.
gpt-4o-mini: USD 0.0005 pro 1 k Input-Token, USD 0.0015 pro 1 k Output-Token.


--> Kosten steuern
Truncate lange Treffer ([:4000] oder SUBSTRING), wie in summarize()
Limit Anzahl Treffer (limit=5).
Dedup identische Stellen, wenn mehrere Suchbegriffe dasselbe Dokument liefern.

Weitere Kosten:
summarize(doc_id) (12 k Zeichen â‰ˆ 1.8 k Tokens) + 120 Antwort-Tokens â‰ˆ 1 900 typische Token ~ USD 0.003
RAG-Chat: 3 Ã— 300 WÃ¶rter Kontext (â‰ˆ 750 Tokens) + Frage (50 Tokens) + Antwort (150 Tokens) â‰ˆ 950 ~ USD 0.0014
https://platform.openai.com/docs/guides/production-best-practices/example-procedure-for-evaluating-a-gpt-3-based-system?utm_source=chatgpt.com

````markdown
## ðŸš€ Quick Start

> **Voraussetzungen**  
> â€“ Docker Desktop â‰¥ 4.x  
> â€“ Git  
> â€“ eigener OpenAI API-Key

```bash
# 1. Repository klonen
git clone https://github.com/<your-org>/vocdata.git
cd vocdata

# 2. API-Key setzen
cp .env.example .env        # Datei anlegen
# â‡’ OPENAI_API_KEY=<dein_Key> in .env eintragen

# 3. Stack starten
docker compose up -d        # baut Image & startet Container
````

**Ã–ffnen:** [http://localhost:8888](http://localhost:8888)
*(Login-Token steht im Container-Log `docker compose logs -f app`.)*

---

## ðŸ”§ Was passiert beim ersten Start?

| Container               | Aufgabe                                                       | Persistenz                |
| ----------------------- | ------------------------------------------------------------- | ------------------------- |
| **db** (`mysql:8.4`)    | legt Schema an, zieht `seed/schema.sql` + `seed/seed.sql` ein | Volume `db_data`          |
| **app** (lokales Build) | installiert Pakete, startet Jupyter Lab :8888                 | Bind-Mount `uploads/pdf/` |

*Die ETL-Pipeline extrahiert Demo-PDFs direkt beim Seed-Import, sodass Dashboards und RAG sofort Daten liefern.*

---

## ðŸ› ï¸ HÃ¤ufige Kommandos

```bash
docker compose ps                  # Container-Status
docker compose logs -f app         # Live-Log Jupyter Lab
docker compose exec db \
  mysql -uroot -pvoc_root vocdata \
  -e "SHOW TABLES;"                # kurzer DB-Check
docker compose down                # Stack stoppen (Volumes bleiben)
```

---

## ðŸ“‚ Verzeichnisstruktur (Auszug)

```text
vocdata/
â”œâ”€ scripts/                # Python-Code (ETL, RAG, Adapter)
â”œâ”€ notebooks/              # Dashboards & Widgets
â”œâ”€ seed/
â”‚   â”œâ”€ schema.sql          # DDL
â”‚   â”œâ”€ seed.sql            # Demo-Kennzahlen
â”‚   â””â”€ demo_pdfs/          # kleine Beispiel-PDFs
â”œâ”€ uploads/pdf/            # eigene PDF-Uploads  (ignored)
â””â”€ docker-compose.yml
```

> **Tipp:**
> *`docker compose down -v`* entfernt Container **und** Volumes, falls Sie komplett neu starten mÃ¶chten.

```
```
