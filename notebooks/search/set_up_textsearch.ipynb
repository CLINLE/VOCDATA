{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c17812",
   "metadata": {},
   "source": [
    "**Embedding-Tabelle qual_chunks anlegen (einmalig) (zum Speichern von embeddings)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b30c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelle qual_chunks angelegt\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "\n",
    "engine = sa.create_engine(\n",
    "    \"mysql+pymysql://root:voc_root@localhost:3306/vocdata?charset=utf8mb4\"\n",
    ")\n",
    "\n",
    "ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS qual_chunks (\n",
    "  chunk_id  INT AUTO_INCREMENT PRIMARY KEY,\n",
    "  doc_id    INT NOT NULL,\n",
    "  chunk_txt TEXT,\n",
    "  embedding BLOB,\n",
    "  FOREIGN KEY (doc_id) REFERENCES qual_docs(doc_id)\n",
    ") ENGINE=InnoDB;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as c:\n",
    "    c.exec_driver_sql(ddl)\n",
    "\n",
    "print(\"Tabelle qual_chunks angelegt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19d2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 0  Pfad-Header\n",
    "import pathlib, sys\n",
    "PROJECT_ROOT = pathlib.Path.cwd().resolve().parents[1]     # …/vocdata\n",
    "SCRIPTS_DIR  = PROJECT_ROOT / \"scripts\"\n",
    "if str(SCRIPTS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SCRIPTS_DIR))\n",
    "\n",
    "from config import OPENAI_API_KEY          # .env → config.py\n",
    "import openai, sqlalchemy as sa, numpy as np, struct, json\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "DB_URL  = \"mysql+pymysql://root:voc_root@localhost:3306/vocdata?charset=utf8mb4\"\n",
    "engine  = sa.create_engine(DB_URL)\n",
    "MODEL   = \"gpt-4.1-mini\"                  # hier Modell wählen\n",
    "EMB_MOD = \"text-embedding-3-small\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0afee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 1  Embedding-Suche + Chat-Antwort\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=256)     # Embedding wird für gleiche Frage wiederverwendet\n",
    "\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    vec = openai.embeddings.create(input=text, model=EMB_MOD).data[0].embedding\n",
    "    return np.array(vec, dtype=np.float32)\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_vectors():\n",
    "    \"\"\"Lädt alle Embeddings einmal in RAM (schneller!).\"\"\"\n",
    "    vecs, meta = [], []\n",
    "    with engine.connect() as c:\n",
    "        for txt, emb, doc, fn in c.execute(sa.text(\"\"\"\n",
    "            SELECT qc.chunk_txt, qc.embedding, qc.doc_id, qd.filename\n",
    "            FROM qual_chunks qc\n",
    "            JOIN qual_docs qd USING (doc_id)\n",
    "        \"\"\")):\n",
    "            vecs.append(np.frombuffer(emb, dtype=np.float32))\n",
    "            meta.append((txt, doc, fn))\n",
    "    mat = np.vstack(vecs)\n",
    "    mat /= np.linalg.norm(mat, axis=1, keepdims=True)   # für Cosine\n",
    "    return mat, meta\n",
    "\n",
    "def top_chunks(question: str, k: int = 4):\n",
    "    q = embed_query(question)\n",
    "    q = q / np.linalg.norm(q)\n",
    "    mat, meta = load_vectors()\n",
    "    sims = mat @ q\n",
    "    best = sims.argsort()[-k:][::-1]\n",
    "    return [meta[i] for i in best]      # [(txt, doc_id, fn), …]\n",
    "\n",
    "def answer(question: str) -> str:\n",
    "    chunks = top_chunks(question, 4)\n",
    "    context = \"\\n\\n\".join(t for t,_,_ in chunks)\n",
    "    prompt  = (\n",
    "        \"Kontext:\\n\" + context +\n",
    "        \"\\n\\nFrage: \" + question +\n",
    "        \"\\nAntworte kurz auf Deutsch:\"\n",
    "    )\n",
    "    rsp = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "    ).choices[0].message.content.strip()\n",
    "\n",
    "    sources = \"\\n\".join(f\"[{i+1}] {fn}\" for i,(_,_,fn) in enumerate(chunks))\n",
    "    return rsp + \"\\n\\n**Quellen**\\n\" + sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d631df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = globals().get('_chat_widget')\n",
    "if old:                       # falls eins aus einem vorigen Lauf existiert\n",
    "    old.close()               # entfernt es aus dem Front-End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452c6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL = gpt-4.1-mini\n",
      "embed_query exists: True\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL =\", MODEL)\n",
    "print(\"embed_query exists:\", callable(globals().get(\"embed_query\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4d25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98acdb4dea14883b73fb13dd50d9030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(border_bottom='1px solid #ccc', border_left='1px solid #ccc', border_right…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_94172\\4070730451.py:91: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  inp.on_submit(lambda _: on_send(None))\n"
     ]
    }
   ],
   "source": [
    "# %% Chat-Widget  – darf beliebig oft ausgeführt werden\n",
    "from IPython.display import display, update_display, Markdown, Javascript\n",
    "import ipywidgets as w, json, pathlib, time, openai, IPython\n",
    "\n",
    "# 1) Browser-DOM von älteren Widgets wegräumen ---------------------\n",
    "Javascript(\"document.querySelectorAll('.voc-chat-widget').forEach(el=>el.remove())\")\n",
    "\n",
    "# 2) Fester Display-Slot leeren (falls schon existiert) ------------\n",
    "SLOT = \"voc_chat_widget\"\n",
    "try:\n",
    "    update_display(None, display_id=SLOT, clear=True)\n",
    "except IPython.core.display.DisplayHandleError:\n",
    "    pass                                      # erster Lauf → Slot fehlte\n",
    "\n",
    "# 3) Verlauf laden --------------------------------------------------\n",
    "PROJECT_ROOT = pathlib.Path.cwd().resolve().parents[1]\n",
    "HIST_FILE    = PROJECT_ROOT / \"chat_history.json\"\n",
    "entries = json.loads(HIST_FILE.read_text()) if HIST_FILE.exists() else []\n",
    "\n",
    "# 4) Widgets bauen --------------------------------------------------\n",
    "hist = w.Output(layout={\n",
    "    'border': '1px solid #ccc', 'padding': '6px',\n",
    "    'height': '300px', 'overflow_y': 'auto', 'white_space': 'pre-wrap'\n",
    "})\n",
    "with hist:\n",
    "    for md in entries:\n",
    "        display(Markdown(md))\n",
    "\n",
    "inp       = w.Text(placeholder=\"Frage …\")\n",
    "send_btn  = w.Button(description=\"⏎ Senden\",  button_style=\"info\")\n",
    "clear_btn = w.Button(description=\"⟲ Verlauf löschen\", button_style=\"danger\")\n",
    "\n",
    "chat_box = w.VBox([hist, w.HBox([inp, send_btn, clear_btn])])\n",
    "chat_box.add_class('voc-chat-widget')\n",
    "display(chat_box, display_id=SLOT)            # genau ein Slot, kein Stapeln\n",
    "\n",
    "# 5) Callback-Funktionen -------------------------------------------\n",
    "def on_send(_):\n",
    "    q = inp.value.strip()\n",
    "    if not q:\n",
    "        return\n",
    "    inp.value = \"\"\n",
    "    entries.extend([f\"**Du:** {q}\", \"_⏳ arbeitet …_\"])\n",
    "    HIST_FILE.write_text(json.dumps(entries, ensure_ascii=False))\n",
    "\n",
    "    with hist:\n",
    "        display(Markdown(entries[-2]))\n",
    "        handle = display(Markdown(entries[-1]), display_id=True)\n",
    "\n",
    "    try:\n",
    "        # 1️⃣ Embedding\n",
    "        t0 = time.perf_counter()\n",
    "        _ = embed_query(q)          # dank LRU meist <0.01 s\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        # 2️⃣ Ranking\n",
    "        chunks = top_chunks(q, 4)\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        # 3️⃣ Chat-Completion (Streaming)\n",
    "        context = \"\\n\\n\".join(t for t,_,_ in chunks)\n",
    "        prompt  = f\"Kontext:\\n{context}\\n\\nFrage: {q}\\nAntworte kurz auf Deutsch:\"\n",
    "        ans_md  = \"\"\n",
    "        stream  = openai.chat.completions.create(\n",
    "                     model=MODEL, stream=True,\n",
    "                     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                  )\n",
    "        for chunk in stream:\n",
    "            ans_md += chunk.choices[0].delta.content or \"\"\n",
    "            handle.update(Markdown(ans_md))\n",
    "        t3 = time.perf_counter()\n",
    "\n",
    "        sources = \"\\n\".join(f\"[{i+1}] {fn}\" for i,(_,_,fn) in enumerate(chunks))\n",
    "        ans = f\"{ans_md}\\n\\n**Quellen**\\n{sources}\"\n",
    "        print(f\"Embedding {t1-t0:.3f}s  Ranking {t2-t1:.3f}s  GPT {t3-t2:.3f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        ans = f\"❌ **Fehler:** {e}\"\n",
    "\n",
    "    entries[-1] = ans\n",
    "    HIST_FILE.write_text(json.dumps(entries, ensure_ascii=False))\n",
    "    handle.update(Markdown(ans))\n",
    "\n",
    "def clear_history(_):\n",
    "    entries.clear()\n",
    "    HIST_FILE.write_text(\"[]\")\n",
    "    hist.clear_output()\n",
    "\n",
    "# 6) Handler binden -------------------------------------------------\n",
    "send_btn.on_click(on_send)\n",
    "inp.on_submit(lambda _: on_send(None))\n",
    "clear_btn.on_click(clear_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vocdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
